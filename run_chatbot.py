# ì±—ë´‡ ì‹¤í–‰ ì‹œë„ìš© íŒŒì¼
import os
import sys
import io
import json
import re
from dotenv import load_dotenv

# LangChain ì„í¬íŠ¸
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage 
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document

# í™”ë©´ ì¶œë ¥ ì¸ì½”ë”© ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding='utf-8')

load_dotenv()

# =========================================================
# [ì„¤ì •] Bi-gram(2ê¸€ì) ë§¤ì¹­ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
# =========================================================
SEARCH_K = 50 
DATA_FILE = "dataset/qa_output/manual_qa_final.json"
CHROMA_DB_PATH = "./chroma_db"

def initialize_system():
    print("ğŸ”„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘... (ë°ì´í„° ë¡œë“œ ë° ê²€ìƒ‰ê¸° ì¤€ë¹„)")
    
    if not os.path.exists(DATA_FILE):
        print("âŒ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    with open(DATA_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)

    docs = []
    for item in data:
        # ì œëª©ê³¼ ì§ˆë¬¸ì„ ê°•ì¡°í•´ì„œ ë‚´ìš© ìƒì„±
        page_content = f"[{item.get('category')}] {item.get('title')}\nQ: {item.get('question')}\nA: {item.get('answer')}"
        metadata = {"source": item.get("source")}
        docs.append(Document(page_content=page_content, metadata=metadata))

    # 1. BM25
    bm25_retriever = BM25Retriever.from_documents(docs)
    bm25_retriever.k = SEARCH_K
    
    # 2. Chroma
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)
    chroma_retriever = vectorstore.as_retriever(search_kwargs={"k": SEARCH_K})
    
    # 3. LLM
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    
    print("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    return bm25_retriever, chroma_retriever, llm

def get_bigrams(text):
    """ë¬¸ìì—´ì„ 2ê¸€ìì”© ì˜ë¼ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ (ì˜ˆ: 'ë°˜ë‚©ì˜' -> ['ë°˜ë‚©', 'ë‚©ì˜'])"""
    text = re.sub(r"\s+", "", text) # ê³µë°± ì œê±°
    return [text[i:i+2] for i in range(len(text) - 1)]

def calculate_match_score(query, doc_content):
    """
    [í•µì‹¬ ë¡œì§] Bi-gram ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
    ì§ˆë¬¸ì˜ 2ê¸€ì ì¡°ê°ë“¤ì´ ë¬¸ì„œì— ì–¼ë§ˆë‚˜ ë§ì´ ë“¤ì–´ìˆëŠ”ì§€ í™•ì¸
    """
    query_bigrams = get_bigrams(query)
    doc_clean = re.sub(r"\s+", "", doc_content)
    
    score = 0
    matched_cnt = 0
    
    for bigram in query_bigrams:
        if bigram in doc_clean:
            matched_cnt += 1
            
            # ì œëª© ë¶€ë¶„(ì• 50ì)ì— ìˆìœ¼ë©´ ê°€ì‚°ì  í­íƒ„
            if bigram in doc_clean[:50]:
                score += 30.0 
            else:
                score += 5.0
    
    return score

def hybrid_search(query, bm25_retriever, chroma_retriever):
    # 1. ê¸°ë³¸ ê²€ìƒ‰ (BM25 + Chroma)
    bm25_res = bm25_retriever.invoke(query)
    chroma_res = chroma_retriever.invoke(query)
    
    score_map = {}
    
    # RRF (ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜)
    for i, doc in enumerate(bm25_res):
        key = doc.page_content
        if key not in score_map: score_map[key] = {'doc': doc, 'score': 0}
        score_map[key]['score'] += (1.0 / (i + 1))

    for i, doc in enumerate(chroma_res):
        key = doc.page_content
        if key not in score_map: score_map[key] = {'doc': doc, 'score': 0}
        score_map[key]['score'] += (1.0 / (i + 1))

    # 2. [í•„ì‚´ê¸°] Bi-gram ë§¤ì¹­ ì ìˆ˜ ì¶”ê°€
    # ì§ˆë¬¸ì— ìˆëŠ” ë‹¨ì–´ ì¡°ê°ì´ ë¬¸ì„œì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì ìˆ˜ë¥¼ íŒíŒ ì¤ë‹ˆë‹¤.
    for key, item in score_map.items():
        match_score = calculate_match_score(query, key)
        item['score'] += match_score

    # 3. ì •ë ¬ ë° Top 3 ì¶”ì¶œ
    sorted_items = sorted(score_map.values(), key=lambda x: x['score'], reverse=True)
    return [item['doc'] for item in sorted_items[:3]]

def generate_answer(query, docs, llm):
    if not docs:
        return "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    context_text = "\n\n".join([f"ë¬¸ì„œ {i+1}:\n{d.page_content}" for i, d in enumerate(docs)])
    
    system_prompt = """
    ë‹¹ì‹ ì€ ì‚¬ë‚´ ê·œì • ì „ë¬¸ê°€ ì±—ë´‡ì…ë‹ˆë‹¤.
    [ê²€ìƒ‰ëœ ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
    
    1. ì§ˆë¬¸ì˜ ì˜ë„(ì •ì˜, ì ˆì°¨, ë°©ë²• ë“±)ì— ê°€ì¥ ì í•©í•œ ë¬¸ì„œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•˜ì„¸ìš”.
    2. 'ë°˜ë‚©'ì— ëŒ€í•´ ë¬¼ì–´ë³´ë©´ 'ë°˜ë‚© ê°œìš”'ë‚˜ 'ë°˜ë‚© ì ˆì°¨' ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.
    3. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë§¤ë‰´ì–¼ì— ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  í•˜ì„¸ìš”.
    """
    
    user_prompt = f"""
    [ê²€ìƒ‰ëœ ë¬¸ì„œ]
    {context_text}

    [ì§ˆë¬¸]
    {query}
    """
    
    response = llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ])
    
    return response.content

def main():
    bm25, chroma, llm = initialize_system()
    
    print("=" * 60)
    print("ğŸ’¡ íŒ: 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ êº¼ì§‘ë‹ˆë‹¤.")
    
    while True:
        try:
            user_input = input("\nğŸ—£ï¸  ì§ˆë¬¸: ")
            
            if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
                print("ğŸ‘‹ ì•ˆë…•íˆ ê°€ì„¸ìš”!")
                break
            
            if not user_input.strip():
                continue

            print("   ğŸ” ë§¤ë‰´ì–¼ ì •ë°€ ê²€ìƒ‰ ì¤‘ (Bi-gram)...")
            
            relevant_docs = hybrid_search(user_input, bm25, chroma)
            
            # [ë””ë²„ê¹…] ì–´ë–¤ ë¬¸ì„œê°€ ë½‘í˜”ëŠ”ì§€ í™•ì¸
            print(f"   ğŸ‘‰ Top 1 ë¬¸ì„œ: {relevant_docs[0].page_content.split('Q:')[0].strip()}")

            print("   ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
            answer = generate_answer(user_input, relevant_docs, llm)
            
            print("\nğŸ“¢ [AI ë‹µë³€]")
            print("-" * 60)
            print(answer)
            print("-" * 60)
            
        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()